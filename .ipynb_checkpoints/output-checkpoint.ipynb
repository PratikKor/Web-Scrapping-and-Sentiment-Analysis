{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678fe8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "\n",
    "# # Load Stop Words\n",
    "# path_to_stopwords = 'path/to/stopwords.txt'\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Load Positive and Negative Dictionaries\n",
    "# path_to_positive_dict = 'path/to/positive_dict.txt'\n",
    "# path_to_negative_dict = 'path/to/negative_dict.txt'\n",
    "\n",
    "# positive_words = set()\n",
    "# negative_words = set()\n",
    "\n",
    "# with open(path_to_positive_dict, 'r') as positive_dict_file:\n",
    "#     positive_words = set(positive_dict_file.read().splitlines())\n",
    "\n",
    "# with open(path_to_negative_dict, 'r') as negative_dict_file:\n",
    "#     negative_words = set(negative_dict_file.read().splitlines())\n",
    "\n",
    "# def clean_text(text):\n",
    "#     # Perform text cleaning\n",
    "#     words = word_tokenize(text)\n",
    "#     cleaned_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "#     return cleaned_words\n",
    "\n",
    "# def calculate_sentiment_scores(cleaned_words):\n",
    "#     # Calculate Positive and Negative Scores\n",
    "#     positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "#     negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "\n",
    "#     # Calculate Polarity Score\n",
    "#     polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "\n",
    "#     # Calculate Subjectivity Score\n",
    "#     subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n",
    "\n",
    "#     return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "\n",
    "# def calculate_readability_metrics(text):\n",
    "#     # Tokenization\n",
    "#     sentences = sent_tokenize(text)\n",
    "#     words = clean_text(text)\n",
    "\n",
    "#     # Calculate Average Sentence Length\n",
    "#     avg_sentence_length = len(words) / len(sentences)\n",
    "\n",
    "#     # Calculate Percentage of Complex Words\n",
    "#     complex_words = [word for word in words if len(word) > 2]\n",
    "#     percentage_complex_words = len(complex_words) / len(words)\n",
    "\n",
    "#     # Calculate Fog Index\n",
    "#     fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "#     # Calculate Average Number of Words Per Sentence\n",
    "#     avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "#     # Calculate Complex Word Count\n",
    "#     complex_word_count = len(complex_words)\n",
    "\n",
    "#     # Calculate Word Count\n",
    "#     word_count = len(words)\n",
    "\n",
    "#     # Calculate Syllable Count Per Word (Simplified for demonstration)\n",
    "#     syllable_count_per_word = sum(len(re.findall('[aeiou]+', word)) for word in words)\n",
    "\n",
    "#     # Calculate Personal Pronouns Count\n",
    "#     personal_pronouns_count = sum(1 for word in words if re.match(r'\\b(?:I|we|my|ours|us)\\b', word, flags=re.IGNORECASE))\n",
    "\n",
    "#     # Calculate Average Word Length\n",
    "#     avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "#     return avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, \\\n",
    "#            complex_word_count, word_count, syllable_count_per_word, personal_pronouns_count, avg_word_length\n",
    "\n",
    "# # Example usage\n",
    "# text_to_analyze = \"Your text goes here.\"\n",
    "\n",
    "# cleaned_words = clean_text(text_to_analyze)\n",
    "# positive_score, negative_score, polarity_score, subjectivity_score = calculate_sentiment_scores(cleaned_words)\n",
    "\n",
    "# avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, \\\n",
    "# complex_word_count, word_count, syllable_count_per_word, personal_pronouns_count, avg_word_length = \\\n",
    "#     calculate_readability_metrics(text_to_analyze)\n",
    "\n",
    "# # Print or use the obtained scores as needed\n",
    "# print(\"Sentiment Scores:\")\n",
    "# print(\"Positive Score:\", positive_score)\n",
    "# print(\"Negative Score:\", negative_score)\n",
    "# print(\"Polarity Score:\", polarity_score)\n",
    "# print(\"Subjectivity Score:\", subjectivity_score)\n",
    "\n",
    "# print(\"\\nReadability Metrics:\")\n",
    "# print(\"Average Sentence Length:\", avg_sentence_length)\n",
    "# print(\"Percentage of Complex Words:\", percentage_complex_words)\n",
    "# print(\"Fog Index:\", fog_index)\n",
    "# print(\"Average Number of Words Per Sentence:\", avg_words_per_sentence)\n",
    "# print(\"Complex Word Count:\", complex_word_count)\n",
    "# print(\"Word Count:\", word_count)\n",
    "# print(\"Syllable Count Per Word:\", syllable_count_per_word)\n",
    "# print(\"Personal Pronouns Count:\", personal_pronouns_count)\n",
    "# print(\"Average Word Length:\", avg_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9cf2ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mayurikor/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bebbb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Scores:\n",
      "Positive Score: 38\n",
      "Negative Score: 24\n",
      "Polarity Score: 0.22580644797086374\n",
      "Subjectivity Score: 0.10316139749889951\n",
      "\n",
      "Readability Metrics:\n",
      "Average Sentence Length: 10.543859649122806\n",
      "Percentage of Complex Words: 1.0\n",
      "Fog Index: 4.617543859649123\n",
      "Average Number of Words Per Sentence: 10.543859649122806\n",
      "Complex Word Count: 601\n",
      "Word Count: 601\n",
      "Syllable Count Per Word: 1741\n",
      "Personal Pronouns Count: 0\n",
      "Average Word Length: 8.312811980033278\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "# import os\n",
    "\n",
    "# # Load Stop Words\n",
    "# stopwords_folder = 'StopWords'\n",
    "# stop_words = set()\n",
    "\n",
    "# for file_name in os.listdir(stopwords_folder):\n",
    "#     with open(os.path.join(stopwords_folder, file_name), 'r', encoding='latin-1') as stopword_file:\n",
    "#         stop_words.update(set(stopword_file.read().splitlines()))\n",
    "\n",
    "# # Load Positive and Negative Dictionaries\n",
    "# positive_words_path = 'MasterDictionary/positive-words.txt'\n",
    "# negative_words_path = 'MasterDictionary/negative-words.txt'\n",
    "\n",
    "# positive_words = set()\n",
    "# negative_words = set()\n",
    "\n",
    "# with open(positive_words_path, 'r', encoding='latin-1') as positive_file:\n",
    "#     positive_words = set(positive_file.read().splitlines())\n",
    "\n",
    "# with open(negative_words_path, 'r', encoding='latin-1') as negative_file:\n",
    "#     negative_words = set(negative_file.read().splitlines())\n",
    "\n",
    "# def clean_text(text):\n",
    "#     # Perform text cleaning\n",
    "#     words = word_tokenize(text)\n",
    "#     cleaned_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "#     return cleaned_words\n",
    "\n",
    "# def calculate_sentiment_scores(cleaned_words):\n",
    "#     # Calculate Positive and Negative Scores\n",
    "#     positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "#     negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "\n",
    "#     # Calculate Polarity Score\n",
    "#     polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "\n",
    "#     # Calculate Subjectivity Score\n",
    "#     subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n",
    "\n",
    "#     return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "\n",
    "# def calculate_readability_metrics(text):\n",
    "#     # Tokenization\n",
    "#     sentences = sent_tokenize(text)\n",
    "#     words = clean_text(text)\n",
    "\n",
    "#     # Calculate Average Sentence Length\n",
    "#     avg_sentence_length = len(words) / len(sentences)\n",
    "\n",
    "#     # Calculate Percentage of Complex Words\n",
    "#     complex_words = [word for word in words if len(word) > 2]\n",
    "#     percentage_complex_words = len(complex_words) / len(words)\n",
    "\n",
    "#     # Calculate Fog Index\n",
    "#     fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "#     # Calculate Average Number of Words Per Sentence\n",
    "#     avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "#     # Calculate Complex Word Count\n",
    "#     complex_word_count = len(complex_words)\n",
    "\n",
    "#     # Calculate Word Count\n",
    "#     word_count = len(words)\n",
    "\n",
    "#     # Calculate Syllable Count Per Word (Simplified for demonstration)\n",
    "#     syllable_count_per_word = sum(len(re.findall('[aeiou]+', word)) for word in words)\n",
    "\n",
    "#     # Calculate Personal Pronouns Count\n",
    "#     personal_pronouns_count = sum(1 for word in words if re.match(r'\\b(?:I|we|my|ours|us)\\b', word, flags=re.IGNORECASE))\n",
    "\n",
    "#     # Calculate Average Word Length\n",
    "#     avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "#     return avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, \\\n",
    "#            complex_word_count, word_count, syllable_count_per_word, personal_pronouns_count, avg_word_length\n",
    "\n",
    "# # Example usage\n",
    "# text_to_analyze = \"Internet Demand’s Evolution, Communication Impact, and 2035’s Alternative Pathways Introduction In the span of just a few decades, the internet has undergone an astounding transformation, becoming an integral part of our lives. As we approach the year 2035, the demand for internet connectivity continues to surge, promising to revolutionize the way we communicate and interact with the world. It also has transformed from a limited communication tool to an all-encompassing global network that shapes our daily lives. As we hurtle toward the year 2035, the trajectory of internet demand and its profound impact on communication is poised to reshape the very foundations of how we connect and interact. This article delves into the intricate interplay between internet demand, communication dynamics, and the alternative pathways that could define our hyper-connected future. The Internet’s Unstoppable Rise The proliferation of smartphones, the Internet of Things (IoT), and the increasing reliance on digital services have driven an exponential increase in Internet demand. This surge is fueled by a growing need for faster data speeds, seamless connectivity, and real-time access to information and services. As the digital divide narrows, more individuals and communities are becoming dependent on the Internet for education, healthcare, commerce, and social interactions. The exponential growth of internet demand has been a defining hallmark of the digital age. From basic web browsing to streaming high-definition content and real-time remote collaborations, the demands placed on internet infrastructure have surged astronomically. This demand is catalyzed by the proliferation of smart devices, the Internet of Things (IoT), and the insatiable appetite for seamless connectivity. Impact on Communications By 2035, the impact of rising internet demand on communication will be multi-faceted. The most notable transformation will likely occur in the realm of high-speed connectivity. The continued development of 5G technology and potentially newer innovations will enable lightning-fast data transfers, virtually eliminating lag and latency. This will redefine remote collaboration, gaming experiences, and real-time communication applications. Moreover, the surge in internet demand will catalyze the evolution of virtual and augmented reality (VR/AR) communication. These technologies will enable individuals to interact as if they were physically present, transcending geographical boundaries and enabling immersive experiences. Teleconferencing, education, and even social gatherings could become more engaging and interactive. As the internet demand continues to rise, the realm of communication is set for a monumental transformation. High-speed connectivity, driven by advancements in 5G and beyond, will render buffering and lag virtually obsolete. This will revolutionize virtual experiences, remote work, and digital entertainment, fostering a sense of immediacy and presence previously unimaginable. The integration of augmented and virtual reality (AR/VR) will further reshape communication dynamics. From immersive virtual meetings to interactive entertainment experiences, AR/VR will bridge physical distances and offer an entirely new dimension of engagement. Challenges and Strain on Infrastructure However, the exponential increase in internet demand brings its own set of challenges. The existing network infrastructure may struggle to keep up with the bandwidth requirements, leading to congestion and reduced quality of service. This could result in a digital divide between urban and rural areas, as well as underserved communities. Moreover, the influx of data traffic raises concerns about data privacy and security. Protecting sensitive information from cyber threats and ensuring the integrity of personal data will become paramount. Striking a balance between seamless connectivity and robust security measures will be a complex task. Challenges in the Hyperconnected Landscape The surge in internet demand, while promising, comes with its own set of challenges. The existing network infrastructure may struggle to accommodate the escalating bandwidth requirements, leading to congestion and inequitable access. Rural areas and underserved communities could face connectivity disparities, potentially exacerbating existing digital divides. Moreover, the influx of data traffic raises concerns about data privacy, security breaches, and the responsible use of emerging technologies. The hyperconnected landscape demands robust measures to protect sensitive information and ensure ethical practices. Exploring Alternatives As the demand for internet connectivity intensifies, alternative communication methods are emerging as potential solutions. Two noteworthy alternatives are decentralized networks and mesh networks. Decentralized networks, often based on blockchain technology, distribute control across multiple nodes, enhancing security and reducing the risk of central points of failure. Mesh networks, on the other hand, operate by connecting devices directly to one another, creating a network without the need for traditional centralized infrastructure. This approach could prove invaluable in areas with limited connectivity or during emergencies when conventional networks are disrupted. In the face of these challenges, alternative pathways are emerging to shape the hyperconnected future. Decentralized networks, underpinned by blockchain technology, offer a promising solution. By distributing control across a network of nodes, these systems enhance security and resilience, reducing vulnerabilities associated with centralized control. Mesh networks, another alternative, challenge the conventional communication paradigm. By establishing direct device-to-device connections, these networks bypass the need for traditional centralized infrastructure, making them particularly valuable in remote or disaster-stricken areas. Conclusion: Forging the Hyperconnected Path Ahead The year 2035 holds the promise of a hyper-connected world, with the surge in internet demand propelling us into a new era of communication. The impact will be transformative, from the widespread adoption of high-speed connectivity to the integration of VR/AR into everyday interactions. Yet, as we navigate this digital frontier, it’s crucial to address the challenges of strained infrastructure, security vulnerabilities, and potential inequalities in access. Exploring alternatives such as decentralized and mesh networks showcase our capacity to innovate and adapt in the face of growing internet demand. As we stride toward 2035, striking the right balance between connectivity, security, and accessibility will shape the future of global communication. As we navigate the hyperconnected future, we stand at a pivotal juncture where our choices will shape the world we inhabit. The rise in internet demand, with its transformative impact on communication, opens the door to unprecedented possibilities. However, the challenges posed by strained infrastructure and security concerns underscore the need for innovative solutions. The alternative pathways of decentralized networks and mesh networks exemplify our capacity to adapt and innovate. The choices we make today will reverberate into 2035 and beyond, defining the contours of a world where connectivity, accessibility, and security converge. As we set our sights on the hyperconnected future, embracing technology’s potential while upholding ethical values will be key to creating a society that thrives in this new era of communication. The journey ahead is one of innovation, collaboration, and responsible stewardship of the digital landscape we collectively shape. Blackcoffer Insights 47: Suhani B Bhatt, Vishwakarma Government Engineering college\"\n",
    "\n",
    "# cleaned_words = clean_text(text_to_analyze)\n",
    "# positive_score, negative_score, polarity_score, subjectivity_score = calculate_sentiment_scores(cleaned_words)\n",
    "\n",
    "# avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, \\\n",
    "# complex_word_count, word_count, syllable_count_per_word, personal_pronouns_count, avg_word_length = \\\n",
    "#     calculate_readability_metrics(text_to_analyze)\n",
    "\n",
    "# # Print or use the obtained scores as needed\n",
    "# print(\"Sentiment Scores:\")\n",
    "# print(\"Positive Score:\", positive_score)\n",
    "# print(\"Negative Score:\", negative_score)\n",
    "# print(\"Polarity Score:\", polarity_score)\n",
    "# print(\"Subjectivity Score:\", subjectivity_score)\n",
    "\n",
    "# print(\"\\nReadability Metrics:\")\n",
    "# print(\"Average Sentence Length:\", avg_sentence_length)\n",
    "# print(\"Percentage of Complex Words:\", percentage_complex_words)\n",
    "# print(\"Fog Index:\", fog_index)\n",
    "# print(\"Average Number of Words Per Sentence:\", avg_words_per_sentence)\n",
    "# print(\"Complex Word Count:\", complex_word_count)\n",
    "# print(\"Word Count:\", word_count)\n",
    "# print(\"Syllable Count Per Word:\", syllable_count_per_word)\n",
    "# print(\"Personal Pronouns Count:\", personal_pronouns_count)\n",
    "# print(\"Average Word Length:\", avg_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e007c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# import re\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# def load_stop_words(stopwords_folder):\n",
    "#     stop_words = set()\n",
    "#     for file_name in os.listdir(stopwords_folder):\n",
    "#         with open(os.path.join(stopwords_folder, file_name), 'r', encoding='latin-1') as stopword_file:\n",
    "#             stop_words.update(set(stopword_file.read().splitlines()))\n",
    "#     return stop_words\n",
    "\n",
    "# def load_dictionary(dictionary_path):\n",
    "#     with open(dictionary_path, 'r', encoding='latin-1') as file:\n",
    "#         return set(file.read().splitlines())\n",
    "\n",
    "# def clean_text(text, stop_words):\n",
    "#     words = word_tokenize(text)\n",
    "#     cleaned_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "#     return cleaned_words\n",
    "\n",
    "# def calculate_sentiment_scores(cleaned_words, positive_words, negative_words):\n",
    "#     positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "#     negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "#     polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "#     subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n",
    "#     return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "\n",
    "# def calculate_readability_metrics(text):\n",
    "#     sentences = sent_tokenize(text)\n",
    "#     words = clean_text(text, stop_words)\n",
    "\n",
    "#     if len(sentences) == 0:\n",
    "#         return 0, 0, 0, 0, 0, 0, 0, 0, 0  # Return zeros if no sentences are present\n",
    "\n",
    "#     avg_sentence_length = len(words) / len(sentences)\n",
    "#     percentage_complex_words = len([word for word in words if len(word) > 2]) / len(words)\n",
    "#     fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "#     avg_words_per_sentence = len(words) / len(sentences)\n",
    "#     complex_word_count = len([word for word in words if len(word) > 2])\n",
    "#     word_count = len(words)\n",
    "#     syllable_count_per_word = sum(len(re.findall('[aeiou]+', word)) for word in words)\n",
    "#     personal_pronouns_count = sum(1 for word in words if re.match(r'\\b(?:I|we|my|ours|us)\\b', word, flags=re.IGNORECASE))\n",
    "#     avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "#     return avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, \\\n",
    "#            complex_word_count, word_count, syllable_count_per_word, personal_pronouns_count, avg_word_length\n",
    "\n",
    "\n",
    "# def process_text_file(file_path, stop_words, positive_words, negative_words):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         text_to_analyze = file.read()\n",
    "\n",
    "#     cleaned_words = clean_text(text_to_analyze, stop_words)\n",
    "#     sentiment_scores = calculate_sentiment_scores(cleaned_words, positive_words, negative_words)\n",
    "#     readability_metrics = calculate_readability_metrics(text_to_analyze)\n",
    "\n",
    "#     return {\n",
    "#         'file_name': os.path.basename(file_path),\n",
    "#         **dict(zip(['positive_score', 'negative_score', 'polarity_score', 'subjectivity_score',\n",
    "#                     'avg_sentence_length', 'percentage_complex_words', 'fog_index', 'avg_words_per_sentence',\n",
    "#                     'complex_word_count', 'word_count', 'syllable_count_per_word', 'personal_pronouns_count',\n",
    "#                     'avg_word_length'], sentiment_scores + readability_metrics))\n",
    "#     }\n",
    "\n",
    "# # Load Stop Words\n",
    "# stop_words = load_stop_words('StopWords')\n",
    "\n",
    "# # Load Positive and Negative Dictionaries\n",
    "# positive_words = load_dictionary('MasterDictionary/positive-words.txt')\n",
    "# negative_words = load_dictionary('MasterDictionary/negative-words.txt')\n",
    "\n",
    "# # Folder containing text files\n",
    "# text_files_folder = 'txt_files'\n",
    "\n",
    "# # Process each text file in the folder\n",
    "# data_list = [process_text_file(os.path.join(text_files_folder, file_name), stop_words, positive_words, negative_words)\n",
    "#              for file_name in os.listdir(text_files_folder) if file_name.endswith('.txt')]\n",
    "\n",
    "# # Create a DataFrame from the list\n",
    "# df = pd.DataFrame(data_list)\n",
    "\n",
    "# # Save the DataFrame to an Excel file\n",
    "# output_excel_path = 'Output_Data_Structure.xlsx'\n",
    "# df.to_excel(output_excel_path, index=False, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9b822d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def load_stop_words(stopwords_folder):\n",
    "    stop_words = set()\n",
    "    for file_name in os.listdir(stopwords_folder):\n",
    "        with open(os.path.join(stopwords_folder, file_name), 'r', encoding='latin-1') as stopword_file:\n",
    "            stop_words.update(set(stopword_file.read().splitlines()))\n",
    "    return stop_words\n",
    "\n",
    "def load_dictionary(dictionary_path):\n",
    "    with open(dictionary_path, 'r', encoding='latin-1') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def clean_text(text, stop_words):\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    return cleaned_words\n",
    "\n",
    "def calculate_sentiment_scores(cleaned_words, positive_words, negative_words):\n",
    "    positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(cleaned_words) + 0.000001)\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "\n",
    "def calculate_readability_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = clean_text(text, stop_words)\n",
    "\n",
    "    if len(sentences) == 0:\n",
    "        return [0] * 9  # Return zeros if no sentences are present\n",
    "\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    percentage_complex_words = len([word for word in words if len(word) > 2]) / len(words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "    complex_word_count = len([word for word in words if len(word) > 2])\n",
    "    word_count = len(words)\n",
    "    syllable_count_per_word = sum(len(re.findall('[aeiou]+', word)) for word in words)\n",
    "    personal_pronouns_count = sum(1 for word in words if re.match(r'\\b(?:I|we|my|ours|us)\\b', word, flags=re.IGNORECASE))\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "    return [avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence,\n",
    "            complex_word_count, word_count, syllable_count_per_word, personal_pronouns_count, avg_word_length]\n",
    "\n",
    "def process_text_file(file_path, stop_words, positive_words, negative_words):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text_to_analyze = file.read()\n",
    "\n",
    "    cleaned_words = clean_text(text_to_analyze, stop_words)\n",
    "    sentiment_scores = calculate_sentiment_scores(cleaned_words, positive_words, negative_words)\n",
    "    readability_metrics = calculate_readability_metrics(text_to_analyze)\n",
    "\n",
    "    return {\n",
    "        'file_name': os.path.basename(file_path),\n",
    "        **dict(zip(['positive_score', 'negative_score', 'polarity_score', 'subjectivity_score',\n",
    "                    'avg_sentence_length', 'percentage_complex_words', 'fog_index', 'avg_words_per_sentence',\n",
    "                    'complex_word_count', 'word_count', 'syllable_count_per_word', 'personal_pronouns_count',\n",
    "                    'avg_word_length'], sentiment_scores + readability_metrics))\n",
    "    }\n",
    "\n",
    "# Load Stop Words\n",
    "stop_words = load_stop_words('StopWords')\n",
    "\n",
    "# Load Positive and Negative Dictionaries\n",
    "positive_words = load_dictionary('MasterDictionary/positive-words.txt')\n",
    "negative_words = load_dictionary('MasterDictionary/negative-words.txt')\n",
    "\n",
    "# Folder containing text files\n",
    "text_files_folder = 'txt_files'\n",
    "\n",
    "# Check if the output Excel file already exists\n",
    "output_excel_path = 'utput.xlsx'\n",
    "if os.path.exists(output_excel_path):\n",
    "    # Load the existing DataFrame\n",
    "    df = pd.read_excel(output_excel_path, engine='openpyxl')\n",
    "else:\n",
    "    # Create an empty DataFrame if the file doesn't exist\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Process each text file in the folder\n",
    "data_list = [process_text_file(os.path.join(text_files_folder, file_name), stop_words, positive_words, negative_words)\n",
    "             for file_name in os.listdir(text_files_folder) if file_name.endswith('.txt')]\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "new_data_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Append new data to the existing DataFrame\n",
    "df = df.append(new_data_df, ignore_index=True)\n",
    "\n",
    "# Save the updated DataFrame to the Excel file\n",
    "df.to_excel(output_excel_path, index=False, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4e931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6ddcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a0f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee2703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731735f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
