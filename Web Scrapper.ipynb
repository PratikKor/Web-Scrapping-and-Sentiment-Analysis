{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cda503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# import os\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from openpyxl import load_workbook\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# import re\n",
    "\n",
    "\n",
    "# #scrapping data from webpage\n",
    "\n",
    "# def fetch_content(url, URL_ID):\n",
    "#     page = requests.get(url)\n",
    "    \n",
    "#     if page.status_code == 200:\n",
    "#         soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#         title = soup.find(\"h1\", {'class': 'entry-title'}) or soup.find(\"h1\", {'class': 'tdb-title-text'})\n",
    "#         article = soup.find(attrs={'class': 'td-post-content tagdiv-type'}) or soup.find(attrs={'class': 'tdb-block-inner td-fix-index'})\n",
    "        \n",
    "#         if title and article:\n",
    "#             title_text = title.text.replace('\\n', ' ')\n",
    "#             article_text = article.text.replace('\\n', ' ')\n",
    "#             file_name = f\"txt_files/{URL_ID}.txt\"\n",
    "#             with open(file_name, \"w\") as file:\n",
    "#                 file.write(title_text)\n",
    "#                 file.write(article_text)\n",
    "#         else:\n",
    "#             print(f\"Unable to extract title or article from {url}\")\n",
    "#             create_empty_file(URL_ID, url)\n",
    "#     else:\n",
    "#         print(f\"Failed to fetch content from {url}\")\n",
    "#         create_empty_file(URL_ID, url)\n",
    "\n",
    "# def create_empty_file(URL_ID, url):\n",
    "#     file_name = f\"txt_files/{URL_ID}.txt\"\n",
    "#     with open(file_name, \"w\") as file:\n",
    "#         file.write(\" \")\n",
    "#     print(f\"Empty file created for {url}\")\n",
    "\n",
    "# df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# # Ensure that the directory 'txt_files' exists, creating it if necessary\n",
    "# if not os.path.exists('txt_files'):\n",
    "#     os.makedirs('txt_files')\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     url = row['URL']\n",
    "#     URL_ID = row['URL_ID']\n",
    "#     fetch_content(url, URL_ID)\n",
    "\n",
    "# print(\"All Data has been Fetched from Webpages\")\n",
    "\n",
    "\n",
    "# #Analysis and adding data to excel\n",
    "\n",
    "# print(\"process started\")\n",
    "# def load_words_from_file(file_path):\n",
    "#     with open(file_path, 'r', encoding='latin-1') as file:\n",
    "#         return set(file.read().splitlines())\n",
    "\n",
    "# def clean_text(text, stop_words):\n",
    "#     return [word.lower() for word in word_tokenize(text) if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "# def calculate_sentiment_scores(cleaned_words, positive_words, negative_words):\n",
    "#     positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "#     negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "#     total_score = positive_score + negative_score\n",
    "    \n",
    "#     subjectivity_score = total_score / len(cleaned_words) if cleaned_words else 0.0\n",
    "#     polarity_score = (positive_score - negative_score) / total_score if total_score else 0.0\n",
    "    \n",
    "#     return positive_score, negative_score, round(polarity_score, 3), round(subjectivity_score, 3)\n",
    "\n",
    "# def calculate_readability_metrics(text, stop_words):\n",
    "#     words = clean_text(text, stop_words)\n",
    "#     sentences = sent_tokenize(text)\n",
    "#     if not sentences:\n",
    "#         return [0] * 9  # Return zeros if there are no sentences\n",
    "#     complex_words = [word for word in words if len(word) > 2]\n",
    "#     avg_sentence_length = len(words) / len(sentences)\n",
    "#     percentage_complex_words = len(complex_words) / len(words)\n",
    "#     fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "#     avg_words_per_sentence = len(words) / len(sentences)\n",
    "#     return [\n",
    "#         round(avg_sentence_length, 3), round(percentage_complex_words, 3), round(fog_index, 3),\n",
    "#         round(avg_words_per_sentence, 3), len(complex_words), len(words),\n",
    "#         sum(len(re.findall('[aeiou]+', word)) for word in words),\n",
    "#         sum(1 for word in words if re.match(r'\\b(?:I|we|my|ours|us)\\b', word, flags=re.IGNORECASE)),\n",
    "#         round(sum(len(word) for word in words) / len(words), 3)\n",
    "#     ]\n",
    "\n",
    "# # Load Stop Words\n",
    "# stop_words = set()\n",
    "# stop_words_path = 'StopWords'\n",
    "# for filename in os.listdir(stop_words_path):\n",
    "#     if filename.endswith('.txt'):\n",
    "#         stop_words.update(load_words_from_file(os.path.join(stop_words_path, filename)))\n",
    "\n",
    "# # Load Positive and Negative Dictionaries\n",
    "# positive_words = load_words_from_file('MasterDictionary/positive-words.txt')\n",
    "# negative_words = load_words_from_file('MasterDictionary/negative-words.txt')\n",
    "\n",
    "# # Load Excel File\n",
    "# file_path = \"Output Data Structure.xlsx\"\n",
    "# wb = load_workbook(filename=file_path)\n",
    "# sheet = wb.active\n",
    "\n",
    "# # Get the last row index\n",
    "# last_row_index = sheet.max_row\n",
    "\n",
    "# # Iterate over each text file in the folder\n",
    "# folder_path = 'txt_files'\n",
    "# for filename in sorted(os.listdir(folder_path)):\n",
    "#     if filename.endswith('.txt'):\n",
    "#         url_id = filename[:-4]  # Remove the \".txt\" extension from the filename\n",
    "#         with open(os.path.join(folder_path, filename), 'r', encoding='latin-1') as file:\n",
    "#             text_to_analyze = file.read()\n",
    "        \n",
    "#         # Perform sentiment analysis and readability calculations\n",
    "#         cleaned_words = clean_text(text_to_analyze, stop_words)\n",
    "#         sentiment_scores = calculate_sentiment_scores(cleaned_words, positive_words, negative_words)\n",
    "#         readability_metrics = calculate_readability_metrics(text_to_analyze, stop_words)\n",
    "        \n",
    "#         # Find the row index where URL_ID matches the filename (without the .txt extension)\n",
    "#         found_row_index = next((row_index for row_index, row in enumerate(sheet.iter_rows(min_row=2, max_row=last_row_index, min_col=1, max_col=1, values_only=True), start=2) if row[0] == url_id), None)\n",
    "        \n",
    "#         if found_row_index is not None:\n",
    "#         # Update the respective columns for the found URL_ID row\n",
    "#             for i, value in enumerate(sentiment_scores + tuple(readability_metrics), start=3):\n",
    "#                 sheet.cell(row=found_row_index, column=i, value=round(value, 3))\n",
    "\n",
    "\n",
    "# print(\"All Files Processed Successfully\")\n",
    "# # Save the updated Excel file\n",
    "# wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content from webpages...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import load_workbook\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "\n",
    "#primary statement for sucesfull launch of program\n",
    "print(\"Fetching content from webpages...\")\n",
    "!pip install pandas\n",
    "#scrapping data from webpages\n",
    "def fetch_content(url, URL_ID):\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        #getting the respective Title and Article text from webpages\n",
    "        title = soup.find(\"h1\", {'class': 'entry-title'}) or soup.find(\"h1\", {'class': 'tdb-title-text'})\n",
    "        article = soup.find(attrs={'class': 'td-post-content tagdiv-type'}) or soup.find(attrs={'class': 'tdb-block-inner td-fix-index'})\n",
    "        \n",
    "        #replacing the blanks and other entities except text and creating .txt files with respective names form URL_ID\n",
    "        if title and article:\n",
    "            title_text = title.text.replace('\\n', ' ')\n",
    "            article_text = article.text.replace('\\n', ' ')\n",
    "            \n",
    "            file_name = f\"txt_files/{URL_ID}.txt\"\n",
    "            with open(file_name, \"w\") as file:\n",
    "                file.write(title_text)\n",
    "                file.write(article_text)\n",
    "            print(f\"File created for {url}\")\n",
    "            \n",
    "        #case handling for empty webpage / data from webpage    \n",
    "        else:\n",
    "            print(f\"Unable to extract title or article from {URL_ID}\")\n",
    "            create_empty_file(URL_ID, url)\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {URL_ID}\")\n",
    "        create_empty_file(URL_ID, url)\n",
    "\n",
    "def create_empty_file(URL_ID, url):\n",
    "    file_name = f\"txt_files/{URL_ID}.txt\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(\" \")\n",
    "    print(f\"Empty file created for {URL_ID}\")\n",
    "\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "#making txt_files folder to store all the .txt files generated\n",
    "os.makedirs('txt_files')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    URL_ID = row['URL_ID']\n",
    "    fetch_content(url, URL_ID)\n",
    "\n",
    "print(\"Data Fetched from Webpages\")\n",
    "\n",
    "print(\"Processing files...\")\n",
    "\n",
    "#Analysis and adding data to excel\n",
    "def load_words_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def clean_text(text, stop_words):\n",
    "    return [word.lower() for word in word_tokenize(text) if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "def calculate_sentiment_scores(cleaned_words, positive_words, negative_words):\n",
    "    positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "    total_score = positive_score + negative_score\n",
    "    \n",
    "    subjectivity_score = total_score / len(cleaned_words) if cleaned_words else 0.0\n",
    "    polarity_score = (positive_score - negative_score) / total_score if total_score else 0.0\n",
    "    \n",
    "    return positive_score, negative_score, round(polarity_score, 3), round(subjectivity_score, 3)\n",
    "\n",
    "def calculate_readability_metrics(text, stop_words):\n",
    "    words = clean_text(text, stop_words)\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return [0] * 9 \n",
    "    complex_words = [word for word in words if len(word) > 2]\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    percentage_complex_words = len(complex_words) / len(words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "    return [\n",
    "        round(avg_sentence_length, 3), round(percentage_complex_words, 3), round(fog_index, 3),\n",
    "        round(avg_words_per_sentence, 3), len(complex_words), len(words),\n",
    "        sum(len(re.findall('[aeiou]+', word)) for word in words),\n",
    "        sum(1 for word in words if re.match(r'\\b(?:I|we|my|ours|us)\\b', word, flags=re.IGNORECASE)),\n",
    "        round(sum(len(word) for word in words) / len(words), 3)\n",
    "    ]\n",
    "\n",
    "#stop words\n",
    "stop_words = set()\n",
    "stop_words_path = 'StopWords'\n",
    "for filename in os.listdir(stop_words_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        stop_words.update(load_words_from_file(os.path.join(stop_words_path, filename)))\n",
    "print(\"Stop words loaded.\")\n",
    "\n",
    "#positive & negative words\n",
    "positive_words = load_words_from_file('MasterDictionary/positive-words.txt')\n",
    "negative_words = load_words_from_file('MasterDictionary/negative-words.txt')\n",
    "print(\"Positive and negative dictionaries loaded.\")\n",
    "\n",
    "#To Load Excel File\n",
    "print(\"Loading Excel file...\")\n",
    "file_path = \"Output Data Structure.xlsx\"\n",
    "wb = load_workbook(filename=file_path)\n",
    "sheet = wb.active\n",
    "\n",
    "last_row_index = sheet.max_row\n",
    "\n",
    "\n",
    "#Looping through each file in txt_file Folder\n",
    "folder_path = 'txt_files'\n",
    "for filename in sorted(os.listdir(folder_path)):\n",
    "    if filename.endswith('.txt'):\n",
    "        #removing the .txt extension so :-4\n",
    "        url_id = filename[:-4]\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='latin-1') as file:\n",
    "            text_to_analyze = file.read()\n",
    "        \n",
    "        # Perform sentiment analysis and readability calculations\n",
    "        cleaned_words = clean_text(text_to_analyze, stop_words)\n",
    "        sentiment_scores = calculate_sentiment_scores(cleaned_words, positive_words, negative_words)\n",
    "        readability_metrics = calculate_readability_metrics(text_to_analyze, stop_words)\n",
    "        \n",
    "        #finding the respective row with URL_ID\n",
    "        found_row_index = next((row_index for row_index, row in enumerate(sheet.iter_rows(min_row=2, max_row=last_row_index, min_col=1, max_col=1, values_only=True), start=2) if row[0] == url_id), None)\n",
    "        \n",
    "        if found_row_index is not None:\n",
    "            #Updating the columns\n",
    "            for i, value in enumerate(sentiment_scores + tuple(readability_metrics), start=3):\n",
    "                sheet.cell(row=found_row_index, column=i, value=round(value, 3))\n",
    "\n",
    "print(\"Saving Excel file...\")\n",
    "wb.save(file_path)\n",
    "print(\"Excel file saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5991d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
