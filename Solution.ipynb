{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4943340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: requests in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: openpyxl in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: nltk in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/mayurikor/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Fetching content from webpages...\n",
      "File created for blackassign0001\n",
      "File created for blackassign0002\n",
      "File created for blackassign0003\n",
      "File created for blackassign0004\n",
      "File created for blackassign0005\n",
      "File created for blackassign0006\n",
      "File created for blackassign0007\n",
      "File created for blackassign0008\n",
      "File created for blackassign0009\n",
      "File created for blackassign0010\n",
      "File created for blackassign0011\n",
      "File created for blackassign0012\n",
      "File created for blackassign0013\n",
      "File created for blackassign0014\n",
      "File created for blackassign0015\n",
      "File created for blackassign0016\n",
      "File created for blackassign0017\n",
      "File created for blackassign0018\n",
      "File created for blackassign0019\n",
      "File created for blackassign0020\n",
      "File created for blackassign0021\n",
      "File created for blackassign0022\n",
      "File created for blackassign0023\n",
      "File created for blackassign0024\n",
      "File created for blackassign0025\n",
      "File created for blackassign0026\n",
      "File created for blackassign0027\n",
      "File created for blackassign0028\n",
      "File created for blackassign0029\n",
      "File created for blackassign0030\n",
      "File created for blackassign0031\n",
      "File created for blackassign0032\n",
      "File created for blackassign0033\n",
      "File created for blackassign0034\n",
      "File created for blackassign0035\n",
      "Failed to fetch content from blackassign0036\n",
      "Empty file created for blackassign0036\n",
      "File created for blackassign0037\n",
      "File created for blackassign0038\n",
      "File created for blackassign0039\n",
      "File created for blackassign0040\n",
      "File created for blackassign0041\n",
      "File created for blackassign0042\n",
      "File created for blackassign0043\n",
      "File created for blackassign0044\n",
      "File created for blackassign0045\n",
      "File created for blackassign0046\n",
      "File created for blackassign0047\n",
      "File created for blackassign0048\n",
      "Failed to fetch content from blackassign0049\n",
      "Empty file created for blackassign0049\n",
      "File created for blackassign0050\n",
      "File created for blackassign0051\n",
      "File created for blackassign0052\n",
      "File created for blackassign0053\n",
      "File created for blackassign0054\n",
      "File created for blackassign0055\n",
      "File created for blackassign0056\n",
      "File created for blackassign0057\n",
      "File created for blackassign0058\n",
      "File created for blackassign0059\n",
      "File created for blackassign0060\n",
      "File created for blackassign0061\n",
      "File created for blackassign0062\n",
      "File created for blackassign0063\n",
      "File created for blackassign0064\n",
      "File created for blackassign0065\n",
      "File created for blackassign0066\n",
      "File created for blackassign0067\n",
      "File created for blackassign0068\n",
      "File created for blackassign0069\n",
      "File created for blackassign0070\n",
      "File created for blackassign0071\n",
      "File created for blackassign0072\n",
      "File created for blackassign0073\n",
      "File created for blackassign0074\n",
      "File created for blackassign0075\n",
      "File created for blackassign0076\n",
      "File created for blackassign0077\n",
      "File created for blackassign0078\n",
      "File created for blackassign0079\n",
      "File created for blackassign0080\n",
      "File created for blackassign0081\n",
      "File created for blackassign0082\n",
      "File created for blackassign0083\n",
      "File created for blackassign0084\n",
      "File created for blackassign0085\n",
      "File created for blackassign0086\n",
      "File created for blackassign0087\n",
      "File created for blackassign0088\n",
      "File created for blackassign0089\n",
      "File created for blackassign0090\n",
      "File created for blackassign0091\n",
      "File created for blackassign0092\n",
      "File created for blackassign0093\n",
      "File created for blackassign0094\n",
      "File created for blackassign0095\n",
      "File created for blackassign0096\n",
      "File created for blackassign0097\n",
      "File created for blackassign0098\n",
      "File created for blackassign0099\n",
      "File created for blackassign0100\n",
      "Data Fetched from Webpages\n",
      "Processing files...\n",
      "Stop words loaded.\n",
      "Positive and negative dictionaries loaded.\n",
      "Loading Excel file...\n",
      "Saving Excel file...\n",
      "Excel file saved.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "required_packages = [\"pandas\", \"requests\", \"beautifulsoup4\", \"openpyxl\", \"nltk\"]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        subprocess.check_call([\"pip\", \"install\", package])\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Failed to install {package}. Please install it manually.\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from openpyxl import load_workbook\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    import re\n",
    "    import os\n",
    "except ImportError as e:\n",
    "    print(f\"Failed to import a required module: {e}\")\n",
    "    print(\"Please make sure all required modules are installed.\")\n",
    "\n",
    "#primary statement for sucesfull launch of program\n",
    "print(\"Fetching content from webpages...\")\n",
    "\n",
    "#scrapping data from webpages\n",
    "def fetch_content(url, URL_ID):\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        #getting the respective Title and Article text from webpages\n",
    "        title = soup.find(\"h1\", {'class': 'entry-title'}) or soup.find(\"h1\", {'class': 'tdb-title-text'})\n",
    "        article = soup.find(attrs={'class': 'td-post-content tagdiv-type'}) or soup.find(attrs={'class': 'tdb-block-inner td-fix-index'})\n",
    "        \n",
    "        #replacing the new lines with whitespace and creating .txt files with respective names form URL_ID\n",
    "        if title and article:\n",
    "            title_text = title.text.replace('\\n', ' ')\n",
    "            article_text = article.text.replace('\\n', ' ')\n",
    "            \n",
    "            file_name = f\"txt_files/{URL_ID}.txt\"\n",
    "            with open(file_name, \"w\") as file:\n",
    "                file.write(title_text)\n",
    "                file.write(article_text)\n",
    "            print(f\"File created for {URL_ID}\")\n",
    "            \n",
    "        #case handling for empty webpage / data from webpage    \n",
    "        else:\n",
    "            print(f\"Unable to extract title or article from {URL_ID}\")\n",
    "            create_empty_file(URL_ID, url)\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {URL_ID}\")\n",
    "        create_empty_file(URL_ID, url)\n",
    "\n",
    "def create_empty_file(URL_ID, url):\n",
    "    file_name = f\"txt_files/{URL_ID}.txt\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(\" \")\n",
    "    print(f\"Empty file created for {URL_ID}\")\n",
    "\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "#making txt_files folder to store all the .txt files generated\n",
    "os.makedirs('txt_files')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    URL_ID = row['URL_ID']\n",
    "    fetch_content(url, URL_ID)\n",
    "\n",
    "print(\"Data Fetched from Webpages\")\n",
    "\n",
    "print(\"Processing files...\")\n",
    "\n",
    "#Analysis and adding data to excel\n",
    "def load_words_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        return set(file.read().splitlines())\n",
    "\n",
    "def clean_text(text, stop_words):\n",
    "    return [word.lower() for word in word_tokenize(text) if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "def calculate_sentiment_scores(cleaned_words, positive_words, negative_words):\n",
    "    positive_score = sum(1 for word in cleaned_words if word in positive_words)\n",
    "    negative_score = sum(1 for word in cleaned_words if word in negative_words)\n",
    "    total_score = positive_score + negative_score\n",
    "    \n",
    "    subjectivity_score = total_score / len(cleaned_words) if cleaned_words else 0.0\n",
    "    polarity_score = (positive_score - negative_score) / total_score if total_score else 0.0\n",
    "    \n",
    "    return positive_score, negative_score, round(polarity_score, 3), round(subjectivity_score, 3)\n",
    "\n",
    "def calculate_readability_metrics(text, stop_words):\n",
    "    words = clean_text(text, stop_words)\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return [0] * 9 \n",
    "    complex_words = [word for word in words if len(word) > 2]\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "    percentage_complex_words = len(complex_words) / len(words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "    return [\n",
    "        round(avg_sentence_length, 3), round(percentage_complex_words, 3), round(fog_index, 3),\n",
    "        round(avg_words_per_sentence, 3), len(complex_words), len(words),\n",
    "        sum(len(re.findall('[aeiou]+', word)) for word in words),\n",
    "        sum(1 for word in words if re.match(r'\\b(?:I|we|my|ours|us)\\b', word, flags=re.IGNORECASE)),\n",
    "        round(sum(len(word) for word in words) / len(words), 3)\n",
    "    ]\n",
    "\n",
    "#stop words\n",
    "stop_words = set()\n",
    "stop_words_path = 'StopWords'\n",
    "for filename in os.listdir(stop_words_path):\n",
    "    if filename.endswith('.txt'):\n",
    "        stop_words.update(load_words_from_file(os.path.join(stop_words_path, filename)))\n",
    "print(\"Stop words loaded.\")\n",
    "\n",
    "#positive & negative words\n",
    "positive_words = load_words_from_file('MasterDictionary/positive-words.txt')\n",
    "negative_words = load_words_from_file('MasterDictionary/negative-words.txt')\n",
    "print(\"Positive and negative dictionaries loaded.\")\n",
    "\n",
    "#To Load Excel File\n",
    "print(\"Loading Excel file...\")\n",
    "file_path = \"Output Data Structure.xlsx\"\n",
    "wb = load_workbook(filename=file_path)\n",
    "sheet = wb.active\n",
    "\n",
    "last_row_index = sheet.max_row\n",
    "\n",
    "\n",
    "#Looping through each file in txt_file Folder\n",
    "folder_path = 'txt_files'\n",
    "for filename in sorted(os.listdir(folder_path)):\n",
    "    if filename.endswith('.txt'):\n",
    "        #removing the .txt extension so :-4\n",
    "        url_id = filename[:-4]\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='latin-1') as file:\n",
    "            text_to_analyze = file.read()\n",
    "        \n",
    "        # Perform sentiment analysis and readability calculations\n",
    "        cleaned_words = clean_text(text_to_analyze, stop_words)\n",
    "        sentiment_scores = calculate_sentiment_scores(cleaned_words, positive_words, negative_words)\n",
    "        readability_metrics = calculate_readability_metrics(text_to_analyze, stop_words)\n",
    "        \n",
    "        #finding the respective row with URL_ID\n",
    "        found_row_index = next((row_index for row_index, row in enumerate(sheet.iter_rows(min_row=2, max_row=last_row_index, min_col=1, max_col=1, values_only=True), start=2) if row[0] == url_id), None)\n",
    "        \n",
    "        if found_row_index is not None:\n",
    "            #Updating the columns\n",
    "            for i, value in enumerate(sentiment_scores + tuple(readability_metrics), start=3):\n",
    "                sheet.cell(row=found_row_index, column=i, value=round(value, 3))\n",
    "\n",
    "print(\"Saving Excel file...\")\n",
    "wb.save(file_path)\n",
    "print(\"Excel file saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d07ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
